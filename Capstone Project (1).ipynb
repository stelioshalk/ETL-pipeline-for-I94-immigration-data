{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "--describe your project at a high level--\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up\n",
    "\n",
    "please refer to https://github.com/stelioshalk/ETL-pipeline-for-I94-immigration-data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "#!pip install pyspark --upgrade\n",
    "import pandas as pd\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "import configparser\n",
    "from subprocess import call\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"PATH\"] = \"/opt/conda/bin:/opt/spark-2.4.3-bin-hadoop2.7/bin:/opt/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/lib/jvm/java-8-openjdk-amd64/bin\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/opt/spark-2.4.3-bin-hadoop2.7\"\n",
    "os.environ[\"HADOOP_HOME\"] = \"/opt/spark-2.4.3-bin-hadoop2.7\"\n",
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg')\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS']['AWS_SECRET_ACCESS_KEY']\n",
    "\n",
    "\n",
    "#create the csv files from the txt files.\n",
    "#call([\"python\", \"generate_csv.py\"])\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "This project is to design a data lake ETL solution that automates data cleansing and processing the I-94 Immigration Dataset. \n",
    "Additionaly, the the Global Temperature Dataset and U.S. City Demographic Dataset are used.\n",
    "\n",
    "#### Describe and Gather Data \n",
    "##### I-94 Dataset\n",
    "For decades, U.S. immigration officers issued the I-94 Form (Arrival/Departure Record) to foreign visitors (e.g., business visitors, tourists and foreign students) who lawfully entered the United States. The I-94 was a small white paper form that a foreign visitor received from cabin crews on arrival flights and from U.S. Customs and Border Protection at the time of entry into the United States. It listed the traveler's immigration category, port of entry, data of entry into the United States, status expiration date and had a unique 11-digit identifying number assigned to it. Its purpose was to record the traveler's lawful admission to the United States.\n",
    "\n",
    "This data is stored as a set of SAS7BDAT files. SAS7BDAT is a database storage file created by Statistical Analysis System (SAS) software to store data. It contains binary encoded datasets used for advanced analytics, business intelligence, data management, predictive analytics, and more. The SAS7BDAT file format is the main format used to store SAS datasets. The immigration data is partitioned into monthly SAS files. The data provided represents 12 months of data for the year 2016. This is the main dataset used in the project.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "###### The I-94 Dataset can be found stored as parquet files in the sas_data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data of the I-94 Dataset. The dataset was read using Spark:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(cicid=5748517.0, i94yr=2016.0, i94mon=4.0, i94cit=245.0, i94res=438.0, i94port='LOS', arrdate=20574.0, i94mode=1.0, i94addr='CA', depdate=20582.0, i94bir=40.0, i94visa=1.0, count=1.0, dtadfile='20160430', visapost='SYD', occup=None, entdepa='G', entdepd='O', entdepu=None, matflag='M', biryear=1976.0, dtaddto='10292016', gender='F', insnum=None, airline='QF', admnum=94953870030.0, fltno='00011', visatype='B1'),\n",
       " Row(cicid=5748518.0, i94yr=2016.0, i94mon=4.0, i94cit=245.0, i94res=438.0, i94port='LOS', arrdate=20574.0, i94mode=1.0, i94addr='NV', depdate=20591.0, i94bir=32.0, i94visa=1.0, count=1.0, dtadfile='20160430', visapost='SYD', occup=None, entdepa='G', entdepd='O', entdepu=None, matflag='M', biryear=1984.0, dtaddto='10292016', gender='F', insnum=None, airline='VA', admnum=94955622830.0, fltno='00007', visatype='B1'),\n",
       " Row(cicid=5748519.0, i94yr=2016.0, i94mon=4.0, i94cit=245.0, i94res=438.0, i94port='LOS', arrdate=20574.0, i94mode=1.0, i94addr='WA', depdate=20582.0, i94bir=29.0, i94visa=1.0, count=1.0, dtadfile='20160430', visapost='SYD', occup=None, entdepa='G', entdepd='O', entdepu=None, matflag='M', biryear=1987.0, dtaddto='10292016', gender='M', insnum=None, airline='DL', admnum=94956406530.0, fltno='00040', visatype='B1'),\n",
       " Row(cicid=5748520.0, i94yr=2016.0, i94mon=4.0, i94cit=245.0, i94res=438.0, i94port='LOS', arrdate=20574.0, i94mode=1.0, i94addr='WA', depdate=20588.0, i94bir=29.0, i94visa=1.0, count=1.0, dtadfile='20160430', visapost='SYD', occup=None, entdepa='G', entdepd='O', entdepu=None, matflag='M', biryear=1987.0, dtaddto='10292016', gender='F', insnum=None, airline='DL', admnum=94956451430.0, fltno='00040', visatype='B1'),\n",
       " Row(cicid=5748521.0, i94yr=2016.0, i94mon=4.0, i94cit=245.0, i94res=438.0, i94port='LOS', arrdate=20574.0, i94mode=1.0, i94addr='WA', depdate=20588.0, i94bir=28.0, i94visa=1.0, count=1.0, dtadfile='20160430', visapost='SYD', occup=None, entdepa='G', entdepd='O', entdepu=None, matflag='M', biryear=1988.0, dtaddto='10292016', gender='M', insnum=None, airline='DL', admnum=94956388130.0, fltno='00040', visatype='B1')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the data here\n",
    "print(\"Sample data of the I-94 Dataset. The dataset was read using Spark:\")\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "df_spark =spark.read.load('./sas_data')\n",
    "\n",
    "df_spark.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Global Temperature Dataset\n",
    "The Berkeley Earth Surface Temperature Study has created a preliminary merged data set by combining 1.6 billion temperature reports from 16 preexisting data archives. \n",
    "The dataset is available here: https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data\n",
    "\n",
    "In the original dataset from Kaggle, several files are available but in this capstone project we will be using only the GlobalLandTemperaturesByCountry.csv.\n",
    "\n",
    "The dataset does not provide tempratures for year 2016. We use this dataset as a dimnsion table linked to the imigration fact table for extracting average temperatures per country. (I94CIT & I94RES fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_c0='dt', _c1='AverageTemperature', _c2='AverageTemperatureUncertainty', _c3='State', _c4='Country'),\n",
       " Row(_c0='1855-05-01', _c1='25.544', _c2='1.171', _c3='Acre', _c4='Brazil'),\n",
       " Row(_c0='1855-06-01', _c1='24.228', _c2='1.103', _c3='Acre', _c4='Brazil'),\n",
       " Row(_c0='1855-07-01', _c1='24.371', _c2='1.044', _c3='Acre', _c4='Brazil'),\n",
       " Row(_c0='1855-08-01', _c1='25.427', _c2='1.073', _c3='Acre', _c4='Brazil')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#run this if you want to vew sample data\n",
    "temperature_fname = 'csvfiles/GlobalLandTemperaturesByState.csv'\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "country_temperature_df=spark.read.csv(temperature_fname)\n",
    "country_temperature_df.head(5)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### The US Cities: Demographics Dataset\n",
    "This dataset contains information about the demographics of all US cities and census-designated places with a population greater or equal to 65,000. This data comes from the US Census Bureau's 2015 American Community Survey. This product uses the Census Bureau Data API but is not endorsed or certified by the Census Bureau. The dataset is available here: https://public.opendatasoft.com/explore/dataset/us-cities-demographics/information/\n",
    "\n",
    "This dataset will be combined with port city data to provide ancillary demographic info for port cities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_c0='City;State;Median Age;Male Population;Female Population;Total Population;Number of Veterans;Foreign-born;Average Household Size;State Code;Race;Count'),\n",
       " Row(_c0='Silver Spring;Maryland;33.8;40601;41862;82463;1562;30908;2.6;MD;Hispanic or Latino;25924'),\n",
       " Row(_c0='Quincy;Massachusetts;41.0;44129;49500;93629;4147;32935;2.39;MA;White;58723'),\n",
       " Row(_c0='Hoover;Alabama;38.5;38040;46799;84839;4819;8229;2.58;AL;Asian;4759'),\n",
       " Row(_c0='Rancho Cucamonga;California;34.5;88127;87105;175232;5821;33878;3.18;CA;Black or African-American;24437')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#run this if you want to vew sample data\n",
    "demographics_fname='csvfiles/us-cities-demographics.csv'\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "demo_df=spark.read.csv(demographics_fname)\n",
    "demo_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Airports Data\n",
    "The airport codes may refer to either IATA airport code, a three-letter code which is used in passenger reservation, ticketing and baggage-handling systems, or the ICAO airport code which is a four letter code used by ATC systems and for airports that do not have an IATA airport code (from wikipedia).\n",
    "\n",
    "Airport codes from around the world. Downloaded from public domain source http://ourairports.com/data/ who compiled this data from multiple different sources. \n",
    "    This data is updated nightly. The definition of the dataset is described here: https://ourairports.com/help/data-dictionary.html\n",
    "    \n",
    "\n",
    "This dataset is in the airport-codes.csv file. This is used for the dimension table \"airports\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_c0='ident', _c1='type', _c2='name', _c3='elevation_ft', _c4='continent', _c5='iso_country', _c6='iso_region', _c7='municipality', _c8='gps_code', _c9='iata_code', _c10='local_code', _c11='coordinates'),\n",
       " Row(_c0='00A', _c1='heliport', _c2='Total Rf Heliport', _c3='11', _c4='NA', _c5='US', _c6='US-PA', _c7='Bensalem', _c8='00A', _c9=None, _c10='00A', _c11='-74.93360137939453, 40.07080078125'),\n",
       " Row(_c0='00AA', _c1='small_airport', _c2='Aero B Ranch Airport', _c3='3435', _c4='NA', _c5='US', _c6='US-KS', _c7='Leoti', _c8='00AA', _c9=None, _c10='00AA', _c11='-101.473911, 38.704022'),\n",
       " Row(_c0='00AK', _c1='small_airport', _c2='Lowell Field', _c3='450', _c4='NA', _c5='US', _c6='US-AK', _c7='Anchor Point', _c8='00AK', _c9=None, _c10='00AK', _c11='-151.695999146, 59.94919968'),\n",
       " Row(_c0='00AL', _c1='small_airport', _c2='Epps Airpark', _c3='820', _c4='NA', _c5='US', _c6='US-AL', _c7='Harvest', _c8='00AL', _c9=None, _c10='00AL', _c11='-86.77030181884766, 34.86479949951172')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#run this if you want to vew sample data\n",
    "airports_fname='csvfiles/airport-codes_csv.csv'\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "airport_df=spark.read.csv(airports_fname)\n",
    "airport_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Additional data\n",
    "A data dictionary file for immigration was provided which was splitted to the following files:\n",
    "\n",
    "- i94cntyl.txt : I94CIT & I94RES - This format shows all the valid and invalid codes for processing \n",
    "- i94prtl.txt  : I94PORT - This format shows all the valid and invalid codes for processing\n",
    "- i94addrl.txt : I94ADDR - There is lots of invalid codes in this variable and the list below shows what we have found to be valid, everything else goes into 'other'\n",
    "- i94model.txt : I94MODE - There are missing values as well as not reported\n",
    "- I94VISA.txt  :I94VISA - Visa codes collapsed into three categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data\n",
    "\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data.\n",
    "\n",
    "Initially we convert the additional data extracted as txt files from the I94_SAS_Labels_Descriptions.SAS file to csv files. \n",
    "\n",
    "Some invalid characters get removed during this process.\n",
    "\n",
    "The script below generates three csv files in the /csvfiles/ folder by reading the source files from the /txtfiles/ folder..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#converts txt files to csvs\n",
    "import os\n",
    "#run once to generate the csv files.\n",
    "#process i94cntyl.txt\n",
    "i94cntylFile = open('txtfiles/i94cntyl.txt', 'r')\n",
    "i94cntylFilecsv = open('csvfiles/i94cntyl.csv', 'w')\n",
    "try:\n",
    "    while True:\n",
    "        line = i94cntylFile.readline() \n",
    "        if not line:\n",
    "            break\n",
    "        list=line.split(\"=\")\n",
    "        newline=list[0].strip()+\",\"+list[1].replace(\"'\",\"\").replace(\",\",\" \")\n",
    "        i94cntylFilecsv.write(newline)\n",
    "except IndexError:\n",
    "    print(\"empty line->\"+line)\n",
    "i94cntylFilecsv.close()\n",
    "\n",
    "#process i94addrl.txt\n",
    "i94addrlFile = open('txtfiles/i94addrl.txt', 'r')\n",
    "i94addrlFilecsv = open('csvfiles/i94addrl.csv', 'w')\n",
    "try:\n",
    "    while True:\n",
    "        line = i94addrlFile.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        list=line.split(\"=\")\n",
    "        newline=list[0].strip().replace(\"'\",\"\")+\",\"+list[1].replace(\"'\",\"\").replace(\",\",\" \")\n",
    "        i94addrlFilecsv.write(newline)\n",
    "except IndexError:\n",
    "    print(\"empty line->\"+line)\n",
    "i94addrlFilecsv.close()\n",
    "\n",
    "#process i94prtl.txt\n",
    "i94prtlFile = open('txtfiles/i94prtl.txt', 'r')\n",
    "i94prtllFilecsv = open('csvfiles/i94prtl.csv', 'w')\n",
    "try:\n",
    "    while True:\n",
    "        line = i94prtlFile.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        list=line.split(\"=\")       \n",
    "        arport_code=list[0].strip().replace(\"'\",\"\")        \n",
    "        secondpart=list[1].split(\",\")        \n",
    "        city=secondpart[0].strip().replace(\"'\",\"\")\n",
    "        if len(secondpart)==2:\n",
    "            state_code=secondpart[1].replace(\"'\",\"\").replace(\" \",\"\")\n",
    "        else:\n",
    "            state_code=\"\"\n",
    "        newline=arport_code+\",\"+city+\",\"+state_code#+\"\\n\"\n",
    "        i94prtllFilecsv.write(newline)\n",
    "except IndexError:\n",
    "    print(\"empty line->\"+line)\n",
    "i94prtllFilecsv.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- Filter temperature data to only use data for United States.\n",
    "- Remove irregular ports from I94 data.\n",
    "- Drop rows with missing IATA codes from I94 data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(dt='1743-11-01', AverageTemperature='10.722000000000001', AverageTemperatureUncertainty='2.898', State='Alabama', Country='United States'),\n",
       " Row(dt='1743-12-01', AverageTemperature=None, AverageTemperatureUncertainty=None, State='Alabama', Country='United States'),\n",
       " Row(dt='1744-01-01', AverageTemperature=None, AverageTemperatureUncertainty=None, State='Alabama', Country='United States'),\n",
       " Row(dt='1744-02-01', AverageTemperature=None, AverageTemperatureUncertainty=None, State='Alabama', Country='United States'),\n",
       " Row(dt='1744-03-01', AverageTemperature=None, AverageTemperatureUncertainty=None, State='Alabama', Country='United States')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Performing cleaning tasks here\n",
    "\n",
    "#Filter GlobalLandTemperaturesByState temperature data to only use data for United States.\n",
    "temperature_fname = 'csvfiles/GlobalLandTemperaturesByState.csv'\n",
    "\n",
    "country_temp_df=spark.read.option(\"header\", \"true\").csv(temperature_fname)\n",
    "filtered_df=country_temp_df.where(\"Country == 'United States'\")\n",
    "\n",
    "#write the filtered dataset as a staging parquet file in S3 bucket\n",
    "filtered_df.write.parquet(path=\"s3a://shalbucket/staging_GlobalLandTemperaturesByState.parquet\", mode = \"overwrite\")\n",
    "filtered_df.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ident='00A', type='heliport', name='Total Rf Heliport', elevation_ft='11', continent='NA', iso_country='US', iso_region='US-PA', municipality='Bensalem', gps_code='00A', iata_code=None, local_code='00A', coordinates='-74.93360137939453, 40.07080078125'),\n",
       " Row(ident='00AA', type='small_airport', name='Aero B Ranch Airport', elevation_ft='3435', continent='NA', iso_country='US', iso_region='US-KS', municipality='Leoti', gps_code='00AA', iata_code=None, local_code='00AA', coordinates='-101.473911, 38.704022'),\n",
       " Row(ident='00AK', type='small_airport', name='Lowell Field', elevation_ft='450', continent='NA', iso_country='US', iso_region='US-AK', municipality='Anchor Point', gps_code='00AK', iata_code=None, local_code='00AK', coordinates='-151.695999146, 59.94919968'),\n",
       " Row(ident='00AL', type='small_airport', name='Epps Airpark', elevation_ft='820', continent='NA', iso_country='US', iso_region='US-AL', municipality='Harvest', gps_code='00AL', iata_code=None, local_code='00AL', coordinates='-86.77030181884766, 34.86479949951172'),\n",
       " Row(ident='00AR', type='closed', name='Newport Hospital & Clinic Heliport', elevation_ft='237', continent='NA', iso_country='US', iso_region='US-AR', municipality='Newport', gps_code=None, iata_code=None, local_code=None, coordinates='-91.254898, 35.6087')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create the other staging parquet files in S3\n",
    "\n",
    "#create the staging_i94prtl\n",
    "staging_i94prtl=spark.read.option(\"header\", \"true\").csv('csvfiles/i94prtl.csv')\n",
    "staging_i94prtl.write.parquet(path=\"s3a://shalbucket/staging_i94prtl.parquet\", mode = \"overwrite\")\n",
    "\n",
    "#create the staging_i94cntyl\n",
    "staging_i94cntyl=spark.read.option(\"header\", \"true\").csv('csvfiles/i94cntyl.csv')\n",
    "staging_i94cntyl.write.parquet(path=\"s3a://shalbucket/staging_i94cntyl.parquet\", mode = \"overwrite\")\n",
    "\n",
    "#create the staging_i94addrl\n",
    "staging_i94addrl=spark.read.option(\"header\", \"true\").csv('csvfiles/i94addrl.csv')\n",
    "staging_i94addrl.write.parquet(path=\"s3a://shalbucket/staging_i94addrl.parquet\", mode = \"overwrite\")\n",
    "\n",
    "#create the staging_i94prtl\n",
    "staging_i94addrl=spark.read.option(\"header\", \"true\").csv('csvfiles/i94prtl.csv')\n",
    "staging_i94prtl.write.parquet(path=\"s3a://shalbucket/staging_i94prtl.parquet\", mode = \"overwrite\")\n",
    "\n",
    "#create the staging_i94model\n",
    "staging_i94model=spark.read.option(\"header\", \"true\").csv('csvfiles/i94model.csv')\n",
    "staging_i94model.write.parquet(path=\"s3a://shalbucket/staging_i94model.parquet\", mode = \"overwrite\")\n",
    "\n",
    "#create the staging_airport_codes\n",
    "staging_airport_codes=spark.read.option(\"header\", \"true\").csv('csvfiles/airport-codes_csv.csv')\n",
    "staging_airport_codes.write.parquet(path=\"s3a://shalbucket/staging_airport_codes.parquet\", mode = \"overwrite\")\n",
    "\n",
    "#create the staging_i94visa\n",
    "staging_i94visa=spark.read.option(\"header\", \"true\").csv('csvfiles/i94visa.csv')\n",
    "staging_i94visa.write.parquet(path=\"s3a://shalbucket/staging_i94visa.parquet\", mode = \"overwrite\")\n",
    "\n",
    "staging_airport_codes.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(cicid=5748517.0, i94yr=2016.0, i94mon=4.0, i94cit=245.0, i94res=438.0, i94port='LOS', arrdate=20574.0, i94mode=1.0, i94addr='CA', depdate=20582.0, i94bir=40.0, i94visa=1.0, count=1.0, dtadfile='20160430', visapost='SYD', occup=None, entdepa='G', entdepd='O', entdepu=None, matflag='M', biryear=1976.0, dtaddto='10292016', gender='F', insnum=None, airline='QF', admnum=94953870030.0, fltno='00011', visatype='B1'),\n",
       " Row(cicid=5748518.0, i94yr=2016.0, i94mon=4.0, i94cit=245.0, i94res=438.0, i94port='LOS', arrdate=20574.0, i94mode=1.0, i94addr='NV', depdate=20591.0, i94bir=32.0, i94visa=1.0, count=1.0, dtadfile='20160430', visapost='SYD', occup=None, entdepa='G', entdepd='O', entdepu=None, matflag='M', biryear=1984.0, dtaddto='10292016', gender='F', insnum=None, airline='VA', admnum=94955622830.0, fltno='00007', visatype='B1'),\n",
       " Row(cicid=5748519.0, i94yr=2016.0, i94mon=4.0, i94cit=245.0, i94res=438.0, i94port='LOS', arrdate=20574.0, i94mode=1.0, i94addr='WA', depdate=20582.0, i94bir=29.0, i94visa=1.0, count=1.0, dtadfile='20160430', visapost='SYD', occup=None, entdepa='G', entdepd='O', entdepu=None, matflag='M', biryear=1987.0, dtaddto='10292016', gender='M', insnum=None, airline='DL', admnum=94956406530.0, fltno='00040', visatype='B1'),\n",
       " Row(cicid=5748520.0, i94yr=2016.0, i94mon=4.0, i94cit=245.0, i94res=438.0, i94port='LOS', arrdate=20574.0, i94mode=1.0, i94addr='WA', depdate=20588.0, i94bir=29.0, i94visa=1.0, count=1.0, dtadfile='20160430', visapost='SYD', occup=None, entdepa='G', entdepd='O', entdepu=None, matflag='M', biryear=1987.0, dtaddto='10292016', gender='F', insnum=None, airline='DL', admnum=94956451430.0, fltno='00040', visatype='B1'),\n",
       " Row(cicid=5748521.0, i94yr=2016.0, i94mon=4.0, i94cit=245.0, i94res=438.0, i94port='LOS', arrdate=20574.0, i94mode=1.0, i94addr='WA', depdate=20588.0, i94bir=28.0, i94visa=1.0, count=1.0, dtadfile='20160430', visapost='SYD', occup=None, entdepa='G', entdepd='O', entdepu=None, matflag='M', biryear=1988.0, dtaddto='10292016', gender='M', insnum=None, airline='DL', admnum=94956388130.0, fltno='00040', visatype='B1')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#filter the i94 dataset \n",
    "#Remove irregular ports from I94 data and write the dataset as a staging parquet file in S3 \n",
    "df_spark =spark.read.load('./sas_data')\n",
    "df_spark.createOrReplaceTempView('raw_immigrations')\n",
    "allowed_ports=spark.read.option(\"header\", \"true\").csv('csvfiles/i94prtl.csv')\n",
    "allowed_ports.createOrReplaceTempView('staging_i94prtl')\n",
    "\n",
    "staging_immigrations_table=spark.sql(\"\"\"\n",
    "SELECT * from raw_immigrations where i94port in (SELECT airport_code from staging_i94prtl)\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "staging_immigrations_table.write.parquet(\"s3a://shalbucket/staging_immigrations.parquet\")\n",
    "staging_immigrations_table.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(city='Silver Spring', state='Maryland', median_age='33.8', male_population='40601', female_population='41862', total_population='82463', number_of_veterans='1562', foreign-born='30908', average_household_size='2.6', state_code='MD', race='Hispanic or Latino', count='25924'),\n",
       " Row(city='Quincy', state='Massachusetts', median_age='41.0', male_population='44129', female_population='49500', total_population='93629', number_of_veterans='4147', foreign-born='32935', average_household_size='2.39', state_code='MA', race='White', count='58723'),\n",
       " Row(city='Hoover', state='Alabama', median_age='38.5', male_population='38040', female_population='46799', total_population='84839', number_of_veterans='4819', foreign-born='8229', average_household_size='2.58', state_code='AL', race='Asian', count='4759'),\n",
       " Row(city='Rancho Cucamonga', state='California', median_age='34.5', male_population='88127', female_population='87105', total_population='175232', number_of_veterans='5821', foreign-born='33878', average_household_size='3.18', state_code='CA', race='Black or African-American', count='24437'),\n",
       " Row(city='Newark', state='New Jersey', median_age='34.6', male_population='138040', female_population='143873', total_population='281913', number_of_veterans='5829', foreign-born='86253', average_household_size='2.73', state_code='NJ', race='White', count='76402')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create the staging_demographics dataset by processing the us-cities-demographics.csv\n",
    "staging_demographics=spark.read.option(\"header\", \"true\").option(\"delimiter\", \";\").csv('csvfiles/us-cities-demographics.csv')\n",
    "\n",
    "#When saving the file to Parquet format, you cannot use spaces and some specific characters.\n",
    "newColumns = []\n",
    "problematic_chars = ',;{}()='\n",
    "for c in staging_demographics.columns:\n",
    "    c = c.lower()\n",
    "    c = c.replace(' ', '_')\n",
    "    for i in problematic_chars:\n",
    "        c = c.replace(i, '')\n",
    "    newColumns.append(c) \n",
    "staging_demographics = staging_demographics.toDF(*newColumns)\n",
    "staging_demographics.write.parquet(path=\"s3a://shalbucket/staging_dmographics.parquet\",mode = \"overwrite\")   \n",
    "\n",
    "staging_demographics.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "The facts table of the proposed model is based on the i94 immigration dataset since this represents the main data that could be analysed. \n",
    "All other datasets are used for populating the dimension tables. \n",
    "\n",
    "The selected data model is composed of the following tables:\n",
    "- immigrations. This table gets populated using data from the  i94 immigration dataset.\n",
    "- country. The country table gets created by using data from the staging_i94cntyl table.\n",
    "- state. The state table gets populated by combining data from the GlobalLandTemperaturesByState and the us-cities-demographics.csv dataset\n",
    "- airport. The airport dimension table is created by processing the airports-codes dataset and the i94prtl.csv.\n",
    "- transportation_mode. This table gets populated by using data from the i94model.csv\n",
    "- visa_status. This table gets populated by using data from the i94visa.csv\n",
    "\n",
    "The data model is displayed in the diagram below:\n",
    "\n",
    "\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model\n",
    "\n",
    "All datasets are processed and cleaned using spark and stored staging parquet files in an S3 bucket.\n",
    "The parquet files get processed further and loaded into a redshift database.\n",
    "\n",
    "\n",
    "The pipeline follows the steps described below. The whole data pipeline can be executed by calling the new_etl.py python script. \n",
    "\n",
    "\n",
    "1. Initially we convert some txt files extracted from the I94_SAS_Labels_Descriptions.SAS file. The txt files get processed, some invalid characters get removed and converted to csv files which are the source of the pipeline in addition to the SAS files.\n",
    "\n",
    "2. All csv files and the SAS files get filtered and stored as staging files in Amazon S3. The staging files are stored as parquet files.\n",
    "3. All existing tables and data get dropped.\n",
    "4. External tables get created which are linked with the parquet files that are stored in S3. Using external tables provides the flexibility to access the data using common sql statements, while at the same time we use the storage capacity of AWS S3 servers.\n",
    "\n",
    "5. The model tables get created.\n",
    "\n",
    "6. The model tables get loaded with data from the external staging tables.\n",
    "\n",
    "The steps 3,4,5 and 6 get executed by calling the create_tables.py. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model. All processes described in this jupiter notebook are included in the new_etl.py file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create the tables in redshift #all sql queries for creating the redshift tables are in sql_queries.py\n",
    "#All existing tables and data get dropped.\n",
    "#External tables get created which are linked with the parquet files that are stored in S3. Using external tables provides the flexibility to access the data using common sql statements, while at the same time we use the storage capacity of AWS S3 servers.\n",
    "#The model tables get created.\n",
    "#The model tables get loaded with data from the external staging tables.\n",
    "#Make sure to update the dl.cfg with the cluster details\n",
    "#Execute the script create_tables.py\n",
    "#Read here for more details about external tables: https://docs.aws.amazon.com/redshift/latest/dg/r_CREATE_EXTERNAL_TABLE.html\n",
    "\n",
    "#create the external staging tables and load the model with data from the staging tables:\n",
    "call([\"python\", \"create_tables.py\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quality check succeeded.\n",
      "Quality check succeeded.\n"
     ]
    }
   ],
   "source": [
    "# Perform quality checks here\n",
    "import psycopg2\n",
    "conn = psycopg2.connect(\"host={} dbname={} user={} password={} port={}\".format(*config['CLUSTER'].values()))\n",
    "conn.set_isolation_level(0) #set the isolation level to 0 or else ddl commands fails such as the create external table \n",
    "cur = conn.cursor()\n",
    "from sql_queries import data_quality_checks_queries\n",
    "for query in data_quality_checks_queries:\n",
    "        cur.execute(query)\n",
    "        result=cur.fetchone()\n",
    "        if result[0]==0:\n",
    "            print(\"Number of rows:0. Quality check failed for query: {}\".format(query))\n",
    "        else:\n",
    "            print (\"Quality check succeeded.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
